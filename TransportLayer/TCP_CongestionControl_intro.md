# 혼잡의 비용과 원인

각 경우에서 왜 혼잡이 발생하는지와 그 혼잡 비용에 대해 먼저 살펴보겠다.  
우리는 아직 혼잡이 어떻게 반응 혹은 회피하는지에 초점을 맞추기 보다는 호스트들이 자신의 전송률을 증가시키고 네트워크 혼잡이 발생함에 따라 무슨일이 일어나는지 쉽게 이해할 수 있는 간단한 문제에 초점을 맞출 것이다.  


## 시나리오 1: 2개의 송신자와 무한 버퍼를 갖는 하나의 라우터

두 호스트 A와 B가 각각 출발지와 목적지 사이에 단일홉을 공유하는 연결을 갖는다고 생각하자.  
호스트 A의 애플리케이션이 X바이트/초의 평균 전송률로 연결상으로 데이터를 보내고 있다고 가정하자.  

이러한 데이터는 각 데이터 단위가 소켓으로 한번만 전송된다는 점에서 원본 데이터이다.  
하위의 트랜스포트 계층 프로토콜은 단순하게 데이터를 캡슐화하고 전송한다.  
오류복구, 흐름제어 또는 혼잡제어를 수행하지 않는다.  
트랜스포트 계층과 하위 계층에서 헤더 정보 추가로 인한 부가적인 오버헤드를 무시하면, 첫 번째 시나리오에서 호스트 A가 라우터에게 제공하는 속도는 X바이트/초이다.  

호스트 B도 비슷한 방식으로 동작하며 설명을 쉽게하기 위해 X바이트/초의 전송률로 데이터를 전송한다고 가정하자.  

호스트 A와 호스트 B가 전송되는 패킷은 라우터와 용량 R의 공유출력 링크를 통과한다.  
라우터는 패킷 도착률이 출력 링크의 용량을 초과하여 입력되는 패킷들을 저장하는 버퍼를 갖고 있다.  
첫 번째 시나리오에서 라우터는 무제한의 버퍼 공간을 갖는다고 가정한다.  

0과 R/2 사이의 전송률에 대해 수신측의 처리량은 송신자의 전송률과 같다.  
송신자가 보내는 모든 데이터는 유한한 지연으로 수신자에게 수신된다.  

그러나 전송률이 R/2 이상일 때 처리량은 단지 R/2에 불과하다.  
처리량에서 이 상한은 두 연결 사이에서 링크 용량 공유에 결과이다.  
링크는 안전상태에서는 R/2를 초과해서 패킷을 수신자에게 전달할 수 없다.  
호스트 A와 B가 그것들의 전송률을 아무리 높게 설정하더라도 각자 R/2보다 더 높은 처리량을 결코 얻을 수 없다.  

R/2의 연결당 처리량을 얻는 것은 목적지에 패킷을 전달하는데 링크를 최대로 활용하므로 좋은 현상이다.  
그러나 전송률이 R/2에 근접했을 때, 평균 지연은 점점 커진다.  

전송률이 R/2를 초과할 때, 라우터 안에 큐잉된 패킷의 평균 개수는 제한되지 않고 출발지와 목적지 사이의 평균 지연이 무제한이된다.  
따라서 R 근처의 전체 처리량에서 동작하는 것은 처리량 관점에서는 이상적이지만, 지연 관점에서는 이상적이지 않다.  

> 심지어 이 이상적인 시나리오에서도 혼잡 네트워크의 한 비용을 발견했다. 즉, 패킷 도착률이 링크 용량에 근접함에 따라 큐잉 지연이 커진다.  


## 시나리오 2: 2개의 송신자, 유한 버퍼를 가진 하나의 라우터  

두 가지 방식으로 시나리오 1을 변경하자.  
첫째, 라우터 버퍼의 양이 유한하다고 가정한다.  
이 현실적인 가정에 따라 이미 버퍼가 가득 찼을 때, 도착하는 패킷들은 버려진다.  

둘째, 각 연결은 신뢰적이라고 가정한다.  
트랜스포트 계층 세그먼트를 포함하는 패킷이 라우터에서 버려지면, 결국 송신자에 의해 재전송 될 것이다. 패킷이 재전송될 수 있으므로, 이제 송신율이라는 용어 사용에 좀 더 주의해야 한다.  

특히 애플리케이션이 원래의 데이터를 소켓으로 보내는 송신율을 X바이트/초로 표기하자 네트워크 안으로 세그먼트를 송신하는 트랜스포트 계층에서의 송신률은 X'바이트/초로 표시하고 X'는 때때로 네트워크에 제공된 부하(Offerd load)라고 부른다.  

시나리오 2에서의 성능은 재전송이 어떻게 수행되는지에 따를 것이다.  
첫째로 호스트 A가 어떻게 해서든 라우터에 있는 버퍼가 비어있는지 그렇지 않은지를 알 수 있고, 버퍼가 비어있을 때만 패킷을 송신할 수 있는 비현실적인 경우를 고려하자.  
이러한 경우 X는 X'와 같으므로 어떠한 손실도 발생하지 않고 연결의 처리량은 X와 같다.  

처리율 관점에서 보면, 송신된 모든 것이 수신되기 때문에 성능은 이상적이다.  
패킷 손실이 절대로 발생하지 않는다고 가정했으므로 이 시나리오에서 평균 호스트 송신율은 R/2를 초과할 수 없다.  

다음은 패킷이 확실히 손실된 것을 알았을 때만 송신자가 재전송하는 좀 더 현실적인 경우를 생각해 보자.  
제공된 부하 X'가 R/2일 경우를 고려해보자.  

이 제공된 부하의 값에서 수신자 애플리케이션으로 전달되는 데이터의 전송률은 R/3이다.  
그러므로 전송된 데이터의 R/2중 0.333R바이트/초는 원래 데이터이고, 0.166R바이트/초는 재전송 데이터이다.  

> 여기서 혼잡 네트워크의 또 다른 비용을 알 수 있다.  
> 즉, 송신자는 버퍼 오버플로 때문에 버려진 패킷을 보상하기 위해 재전송을 수행해야 한다.  

마지막으로 송신자에서 너무 일찍 타임아웃되어 패킷이 손실되지 않았지만 큐에 지연되고 있는 패킷을 재전송하는 경우를 생각해보자.  

이럴 때 원래 데이터 패킷과 재전송 패킷 둘다 수신자에게 도착한다.  
물론 수신자는 단지 하나의 패킷 복사본만 필요하므로 재전송된 패킷을 버린다.  
이런 경우 수신자가 이미 패킷의 원래 복사본을 수신했을 때 라우터에서 원래 패킷의 재전송된 복사본을 전달하는 작업은 낭비이다.  

> 따라서 여기에 혼잡 네트워크의 또다른 비용이 있다.  
> 즉, 커다란 지연으로 인한 송신자의 불필요한 재전송은 라우터가 패킷의 불필요한 복사본을 전송하는 원인이된다.  

각 패킷이 두 번씩 전달되므로 제공된 부하가 R/2일 때 처리량은 R/4가 될 것이다.  

## 시나리오 3: 4개의 송신자와 유한 버퍼를 갖는 라우터, 그리고 멀티홉 경로

마지막 혼잡 시나리오에서 4개의 호스트는 겹쳐지는 2홉 경로를 통해 패킷을 전송한다.  
다시 각각의 호스트가 안정적인 데이터 전송 서비스를 실행하기 위해 타임아웃/재전송 매커니즘을 사용한다고 가정한다.  
모든 호스트는 X의 동일한 값을 갖고 모든 라우터의 링크는 R바이트/초 용량을 갖는다.  

라우터 R1과 R2를 지나가는 호스트 A에서 호스트 C까지의 연결을 고려하자.  
A~ C 연결은 D~B 연결과 라우터 R1을 공유한다.  
그리고 D~B 연결과 라우터 R2를 공유한다.  

X의 극히 작은 값에 대해 버퍼 오버플로는 겅의 발생하지 않고, 혼잡 시나리오1과 2처럼 처리량은 제공된 부하와 겅의 같다.  
X의 약간 더 큰 값에 대해 원래의 데이터가 네트워크로 전송되고 목적지에 전달되고 오버플로가 거의 발생하지 않으므로 해당하는 처리량은 약간 커진다.  

그러므로 X가 작은 값일 때 X의 증가는 Xout의 증가를 가져온다.  

지금까지는 극히 작은 트래픽의 경우를 살펴보았고 이제 X값이 매우 큰 경우를 살펴보자.  
라우터 R2에 도착하는 A~C 트래픽은 R2에서 X의 값에 관계없이 R1에서 R2까지의 링크 용량 최대 R인 도착률을 가질 수 있다.  

만약 X가 모든 연결에 대해 매우 크다면 R2의 B~D 트래픽 도착률은 A~C 트래픽 도착률보다 클 수 있다.  
A~C와 B~D 트래픽은 버퍼 공간을 R2에서 경쟁해야 하므로 R2를 성공적으로 통과하는 A~C 트래픽의 양은 B~D에서 제공된 부하가 크다면 클수록 더 작아진다.  

한계 내에서, 제공된 부하가 무한대에 가까워지면 R2의 빈 버퍼는 즉시 B~D 패킷으로 채워지고 R2에서 A~C 연결의 처리량은 0이된다.  
이것은 트래픽이 많은 경우 A~C 종단간 처리율이 0이 된다는 것을 의미한다.  
제공된 부하와 처리량 간의 트레이드 오프가 발생함을 보여준다.  

제공된 부하를 증가시키면 처리량이 궁극적으로 감소하는 원인은 네트워크가 수행한 헛된 '작업'의 양을 고려해보면 확실해진다.  
앞에서 설명한 과도한 트래픽 시나리오에서 패킷이 두 번째 홉 라우터에서 버려질 때 마다 두 번째 홉 라우터에서 패킷을 전달하는 첫 번째 홉 라우터에서 수행된 '작업'은 헛된 것이 된다.  

만약 첫 번째 라우터가 간단하게 그 패킷을 포기하고 유휴상태를 유지한다면, 네트워크는 그 만큼 잘 돌아갈 것이다.  
더 중요한 점은 두 번째 라우터에게 패킷을 전달하기 위해 첫 번째 라우터에서 사용되는 전송 용량은 다른 패킷을 전송하기 위해 좀 더 유익하게 사용될 수 있다는 것이다.  

> 여기서 혼잡 때문에 패킷을 버려야하는 또 다른 비용을 확인할 수 있다.  
> 패킷이 경로상에서 버려질 때, 버려지는 지점까지 패킷을 전송하는데 사용된 상위 라우터에서 사용된 전송 용량은 낭비된 것이다.  


# 혼잡제어에 대한 접근법

종단 간의 혼잡제어: 네트워크 계층은 혼잡제어 목적을 위해 트랜스포트 계층에게 어떤 직접적인 지원도 제공하지 않는다.  
혼잡의 존재는 단지 관찰된 네트워크 동작에 기초하여 종단 시스템이 추측해야 한다.  
IP 계층이 네트워크 동작에 관해 종단 시스템에게 어떠한 피드백 제공도 요구받지 않으므로 TCP가 혼잡 제어를 위한 종단간의 접근방식을 취한다는것을 볼 수 있다.  

TCP 세그먼트 손실은 네트워크 혼잡의 발생 표시로 간주하고 TCP는 그에 따라서 윈도 크기를 줄인다.  
또한 TCP에 대한 새로운 제안으로, 증가하는 왕복 지연 값을 네트워크 혼잡 증가 지표로 사용하는 것을 보게 될 것이다.  

네트워크 지원 혼잡 제어: 라우터들은 네트워크 안에서 혼잡 생타와 관련하여 송신자나 수신자 또는 모두에게 직접적인 피드백을 제공한다.  
이 피드백은 링크의 혼잡을 나타내는 하나의 비트처럼 간단하다.  
즉, ATM ABR 혼잡제어에서 라우터는 자신이 출력 링크에 제공할 수 있는 전송률을 송신자에게 명확히 알릴 수 있게 해준다.  

인터넷 기본 버전의 IP 및 TCP는 혼잡제어에 대해 종단 간 접근 방식을 채택했으나 최근에는 네트워크 혼잡제어를 선택적으로 구현할 수 있다.  

직접 피드백은 네트워크 라우터에서 송신자에게 보내는 것이다.  
이 알림의 형태는 전형적으로 초크 패킷의 형태를 갖는다. 즉, "나는 혼잡하다"라고 말하는 것이다.  

알림의 두 번째 형태는 라우터가 혼잡을 나타내기 위해 송신자에서 수신자에게로 흐르는 패킷 안의 특정 필드에 표시/수정 하는 것이다.  
수신자가 표시된 패킷을 수신했을 때 혼잡 상태를 송신자에게 알린다.  
후자 형태의 알림은 완전한 왕복 시간이 걸린다는 것에 유의해야 된다.  








